{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation and benchmarking\n",
    "\n",
    "In this notebook we will go step-by-step through the model evaluation part of our paper, as well as through the benchmarking part, where we compare our simulation results with a good hand-full of different (well-established) hydrological models.\n",
    "\n",
    "For more information read the experiment description in our paper:\n",
    "\n",
    "**TODO**: Include Ref\n",
    "\n",
    "Note:\n",
    "If you want to run this notebook locally and reproduce the figures of our paper\n",
    "- make sure you have our pre-trained models. See the [README.md](link) in the repository for further instructions.\n",
    "\n",
    "- make sure to have the CAMELS benchmark data set. See the [README.md](link) in the repository for further instructions.\n",
    "\n",
    "#### Adapt the lines below according to your local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed if no precomputed results are used. Main directory containing all runs\n",
    "BASE_RUN_DIR = \"/home/mgauch/ealstm_regional_modeling/runs/\"\n",
    "\n",
    "# Path to the benchmark model folders containing the basin netCDF files\n",
    "BENCHMARK_DIR = \"/home/mgauch/ealstm_regional_modeling/data/netcdf/\"\n",
    "\n",
    "# Path to the main directory of this repository\n",
    "BASE_CODE_DIR = \"/home/mgauch/ealstm_regional_modeling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.append(BASE_CODE_DIR)\n",
    "from papercode.plotutils import model_draw_style, model_specs, ecdf\n",
    "from papercode.evalutils import (get_run_dirs, eval_datadriven_models, \n",
    "                                 eval_benchmark_models, get_pvals, \n",
    "                                 get_mean_basin_performance, get_cohens_d)\n",
    "from papercode.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of global variables. \n",
    "\n",
    "**Note**: If you want to recompute all model metrics, change the `PRECOMPUTED_DATA` flag to `False`. No GPU is required. If you want to use the pre-calculated metrics, make sure the flag is set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True load pre-computed metrics from pickle file, else re-calculate everything\n",
    "PRECOMPUTED_DATA = False\n",
    "\n",
    "# Convert to PosixPaths\n",
    "BASE_RUN_DIR = Path(BASE_RUN_DIR)\n",
    "BENCHMARK_DIR = Path(BENCHMARK_DIR)\n",
    "\n",
    "# Set of evaluation functions\n",
    "EVAL_FUNCS = {'NSE': calc_nse, \n",
    "              'alpha_nse': calc_alpha_nse, \n",
    "              'beta_nse': calc_beta_nse,\n",
    "              'FHV': calc_fdc_fhv, \n",
    "              'FLV': calc_fdc_flv, \n",
    "              'FMS': calc_fdc_fms}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate data-driven models\n",
    "\n",
    "First, we compare the XGBoost model with the 6 different settings of LSTM-based models, that we trained for this paper. These are three different model settings:\n",
    "\n",
    "1. EA-LSTM: Our proposed adaption of the LSTM recurrent neural network, where the static catchment characteristics are used to modulate the input gate.\n",
    "2. LSTM: The standard LSTM architecture, where the static catchment characteristics are concatenated to the meterological inputs at each time step.\n",
    "3. LSTM (no static inputs): A standard LSTM that is only trained using the meteorological forcing data.\n",
    "\n",
    "All three model configurations were trained using two different loss functions:\n",
    "\n",
    "1. MSELoss: The standard mean squared error loss.\n",
    "2. NSELoss: Our proposed loss function, which approximates the basin averaged NSE. For more details see the Method section of our manuscript.\n",
    "\n",
    "For each of the 6 settings, we trained 8 models (using different random initializations) and furthermore combined these 8 models to an ensemble (by averaging the k=8 model simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PRECOMPUTED_DATA:\n",
    "    print(\"Loaded data from pre-computed pickle file\")\n",
    "    with open(\"all_metrics.p\", \"rb\") as fp:\n",
    "        all_metrics = pickle.load(fp)\n",
    "else:\n",
    "    all_metrics = {}\n",
    "    for func_name, func in EVAL_FUNCS.items():\n",
    "        tqdm.tqdm.write(f\"Calculating metric: {func_name}\")\n",
    "        model_metrics = {}\n",
    "        for model, specs in model_specs.items():\n",
    "            run_dirs = get_run_dirs(root_dir=BASE_RUN_DIR, model=specs[\"model\"], loss=specs[\"loss\"])\n",
    "            model_metrics[model] = eval_datadriven_models(run_dirs=run_dirs, func=func)\n",
    "\n",
    "        all_metrics[func_name] = model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular comparison\n",
    "\n",
    "In a first step, we look at the mean, median NSE as well as the number of catastrophic failures. Catastrophic failures are defined as the number of basins, where the model has a NSE <= 0.\n",
    "We calculate the mean of each of this three statistics over the model (n=8) model repetitions and report the standard deviation here as well. The repetitions are denoted as `ensemble=False` in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for model_type, models in all_metrics[\"NSE\"].items():\n",
    "    if model_type == \"benchmarks\":\n",
    "        continue\n",
    "    seeds = [k for k in models.keys() if \"seed\" in k]\n",
    "    means, medians, failures = [], [], []\n",
    "    for seed in seeds:\n",
    "        nses = list(models[seed].values())\n",
    "        means.append(np.mean(nses))\n",
    "        medians.append(np.median(nses))\n",
    "        failures.append(len([v for v in nses if v <= 0]))\n",
    "    data_sing = {'model_type': model_draw_style[model_type][\"label\"], \n",
    "                 'ensemble': False, \n",
    "                 'mean': np.mean(means), \n",
    "                 'std_mean': np.std(means),\n",
    "                 'median': np.mean(medians),\n",
    "                 'std_median': np.std(medians),\n",
    "                 'failures': np.mean(failures),\n",
    "                 'std_failures': np.std(failures)}\n",
    "    data.append(data_sing)\n",
    "    values = list(models[\"ensemble\"].values())\n",
    "    data_ensemble = {'model_type': model_draw_style[model_type][\"label\"],\n",
    "                   'ensemble': True,\n",
    "                   'mean': np.mean(values),\n",
    "                   'median': np.median(values),\n",
    "                   'failures': len([v for v in values if v < 0]) }\n",
    "    data.append(data_ensemble)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(keys=[\"model_type\", \"ensemble\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative density function plot\n",
    "\n",
    "Here we look at the CDF of the NSEs for each of our 6 configurations. \n",
    "\n",
    "- `solid` lines mark models trained with `NSELoss`, `dashed` lines mark models trained with `MSELoss`\n",
    "- `saturated colors` are ensemble (n=8) means, `non-saturated colors` is a single model. Here we took seed 111 of each model, which is rather arbitrary but from the table above we seed that the mean/median NSE are robust between different random initializations.\n",
    "- `square` marker denote models trained with static features, while `triangle` markers denote models trained without static features\n",
    "- `green` is our proposed `EA-LSTM`, `orange` the standard LSTM with static features and `purple` the standard LSTM trained only on meteorological forcing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "for model_type, models in all_metrics[\"NSE\"].items():\n",
    "    if 'lstm' in model_type or 'xgboost' in model_type:\n",
    "        # single seed\n",
    "        values = list(models['seed111'].values())\n",
    "        bin_, cdf_ = ecdf(values)\n",
    "        ax.plot(bin_,\n",
    "                cdf_,\n",
    "                label=f\"{model_draw_style[model_type]['label']} seed111\",\n",
    "                color=model_draw_style[model_type][\"single_color\"], \n",
    "                marker=model_draw_style[model_type]['marker'], \n",
    "                markevery=20, \n",
    "                linestyle=model_draw_style[model_type]['linestyle'])\n",
    "        \n",
    "        # ensemble seed\n",
    "        values = list(models['ensemble'].values())\n",
    "        bin_, cdf_ = ecdf(values)\n",
    "        ax.plot(bin_,\n",
    "                cdf_, \n",
    "                label=f\"{model_draw_style[model_type]['label']} ensemble (n=8)\", \n",
    "                color=model_draw_style[model_type]['ensemble_color'], \n",
    "                linestyle=model_draw_style[model_type]['linestyle'])\n",
    "    \n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('NSE', fontsize=14)\n",
    "ax.set_ylabel('cumulative density', fontsize=14)\n",
    "ax.set_title(\"Effect of (not) using static catchment attributes\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate statistical significance.\n",
    "\n",
    "1. Calculate statistical significance between LSTM trained with and without static features using either MSE or NSE as loss function.\n",
    "\n",
    "2. Calculate statistical significance between EA-LSTM and standard LSTM (with static features)\n",
    "\n",
    "We always report the max, mean p-value between all possible seed combinations (n=8^2=64) as well as the p-value between the ensemble means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### NSE:\")\n",
    "print(\"XGBoost with statics, optimized with MSE or LSTM with statics, optimized with MSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='xgboost_MSE', \n",
    "                                         model2='lstm_MSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")\n",
    "\n",
    "print(\"\\nWith or without statics, optimized with MSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='lstm_no_static_MSE', \n",
    "                                         model2='lstm_MSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")\n",
    "\n",
    "print(\"\\nWith or without statics, optimized with NSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='lstm_no_static_NSE', \n",
    "                                         model2='lstm_NSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")\n",
    "\n",
    "print(\"\\nEA-LSTM vs LSTM (with statics), optimized with MSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='lstm_MSE', \n",
    "                                         model2='ealstm_MSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")\n",
    "\n",
    "print(\"\\nEA-LSTM vs LSTM (with statics), optimized with NSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='lstm_NSE', \n",
    "                                         model2='ealstm_NSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")\n",
    "\n",
    "values1 = get_mean_basin_performance(all_metrics[\"NSE\"], model=\"ealstm_NSE\")\n",
    "values1 = list(values1.values())\n",
    "values2 = get_mean_basin_performance(all_metrics[\"NSE\"], model=\"lstm_NSE\")\n",
    "values2 = list(values2.values())\n",
    "d = get_cohens_d(values1, values2)\n",
    "print(f\"Effect size using Cohen's d is: d={d:.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\nEA-LSTM optimized with NSE vs. EA-LSTM optimized with MSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='ealstm_NSE', \n",
    "                                         model2='ealstm_MSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")\n",
    "\n",
    "print(\"\\nLSTM  without static features optimized with NSE vs. optimized with MSE\")\n",
    "p_val_single, p_val_ensemble = get_pvals(all_metrics[\"NSE\"], \n",
    "                                         model1='lstm_no_static_MSE', \n",
    "                                         model2='lstm_no_static_NSE')\n",
    "print(f\"Single models: p-value={p_val_single:.2e}\")\n",
    "print(f\"Ensemble mean: {p_val_ensemble:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against benchmark models\n",
    "\n",
    "Now we compare our model, the `EA-LSTM` optimized with `NSELoss`, and `XGBoost`, against the set of benchmark models. Here, we only use the model results from the set of basins that were modeled by all models (the benchmark models and our models).\n",
    "\n",
    "First, we have to calculate the metrics for all basins and benchmark models (or load the data from the precomputed file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED_DATA:\n",
    "    for metric in all_metrics.keys():\n",
    "        tqdm.tqdm.write(f\"Calculating metric: {metric}\")\n",
    "        all_metrics[metric][\"benchmarks\"] = eval_benchmark_models(netcdf_folder=BENCHMARK_DIR, \n",
    "                                                                  func=EVAL_FUNCS[metric])\n",
    "else:\n",
    "    print(\"Evaluation data of benchmark models already exist in pre-computed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED_DATA:\n",
    "    with open(\"all_metrics.p\", \"wb\") as fp:\n",
    "        pickle.dump(all_metrics, fp)\n",
    "    print(\"Stored precomputed data in 'all_metrics.p'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all basins modeled by all benchmarks\n",
    "basins = frozenset(list(all_metrics[\"NSE\"][\"ealstm_NSE\"][\"ensemble\"].keys()))\n",
    "basins = basins.intersection(all_metrics[\"NSE\"][\"xgboost_MSE\"][\"ensemble\"].keys())\n",
    "for model, results in all_metrics[\"NSE\"][\"benchmarks\"].items():\n",
    "    basins = basins.intersection(list(results.keys()))\n",
    "len(basins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of all metrics for these share basins\n",
    "sub_metrics = {metric: defaultdict(dict) for metric in all_metrics.keys()}\n",
    "for metric, model_metric in all_metrics.items():\n",
    "    for model_type, models in model_metric.items():\n",
    "        for model, results in models.items():\n",
    "            sub_metrics[metric][model_type][model] = {}\n",
    "            for basin, nse in results.items():\n",
    "                if basin in basins:\n",
    "                    sub_metrics[metric][model_type][model][basin] = nse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "for model_type, models in sub_metrics[\"NSE\"].items():\n",
    "    if (model_type == \"ealstm_NSE\") or (model_type == \"lstm_no_static_NSE\") or (model_type == \"xgboost_MSE\"):\n",
    "        # single seed\n",
    "        values = list(models['seed111'].values())\n",
    "        bin_, cdf_ = ecdf(values)\n",
    "        ax.plot(bin_,\n",
    "                cdf_,\n",
    "                label=f\"{model_draw_style[model_type]['label']} seed111\",\n",
    "                color=model_draw_style[model_type][\"single_color\"], \n",
    "                marker=model_draw_style[model_type]['marker'], \n",
    "                markevery=20, \n",
    "                linestyle=model_draw_style[model_type]['linestyle'])\n",
    "        \n",
    "        # ensemble seed\n",
    "        values = list(models['ensemble'].values())\n",
    "        bin_, cdf_ = ecdf(values)\n",
    "        ax.plot(bin_,\n",
    "                cdf_, \n",
    "                label=f\"{model_draw_style[model_type]['label']} ensemble (n=8)\", \n",
    "                color=model_draw_style[model_type]['ensemble_color'], \n",
    "                linestyle=model_draw_style[model_type]['linestyle'])\n",
    "    elif model_type == \"benchmarks\":\n",
    "        for benchmark_model, benchmark_result in models.items():\n",
    "            if \"conus\" in benchmark_model:\n",
    "                values = list(benchmark_result.values())\n",
    "                bin_, cdf_ = ecdf(values)\n",
    "                ax.plot(bin_,\n",
    "                        cdf_, \n",
    "                        label=model_draw_style[benchmark_model]['label'], \n",
    "                        color=model_draw_style[benchmark_model]['color'], \n",
    "                        linestyle=model_draw_style[benchmark_model]['linestyle'])\n",
    "    \n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('NSE', fontsize=14)\n",
    "ax.set_ylabel('cumulative density', fontsize=14)\n",
    "ax.set_title(\"Benchmarking against CONUS-wide calibrated hydrological models\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_count = 0\n",
    "mhm_count = 0\n",
    "xgb_count = 0\n",
    "for basin in basins:\n",
    "    lstm_nse = sub_metrics[\"NSE\"][\"ealstm_NSE\"][\"ensemble\"][basin]\n",
    "    if sub_metrics[\"NSE\"][\"benchmarks\"][\"VIC_conus\"][basin] >= lstm_nse:\n",
    "        vic_count += 1\n",
    "    if sub_metrics[\"NSE\"][\"benchmarks\"][\"mHm_conus\"][basin] >= lstm_nse:\n",
    "        mhm_count += 1\n",
    "    if sub_metrics[\"NSE\"][\"xgboost_MSE\"][\"ensemble\"][basin] >= lstm_nse:\n",
    "        xgb_count += 1\n",
    "        \n",
    "print(f\"VIC is better (or equal) than EA-LSTM ensemble mean in {vic_count}/{len(basins)} basins\")\n",
    "print(f\"mHm is better (or equal) than EA-LSTM ensemble mean in {mhm_count}/{len(basins)} basins\")\n",
    "print(f\"XGBoost is better (or equal) than EA-LSTM ensemble mean in {xgb_count}/{len(basins)} basins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "\n",
    "for model_type, models in sub_metrics[\"NSE\"].items():\n",
    "    if model_type == \"ealstm_NSE\" or model_type == \"xgboost_MSE\":\n",
    "        # single seed\n",
    "        values = list(models['seed111'].values())\n",
    "        bin_, cdf_ = ecdf(values)\n",
    "        ax.plot(bin_,\n",
    "                cdf_,\n",
    "                label=f\"{model_draw_style[model_type]['label']} seed111\",\n",
    "                color=model_draw_style[model_type][\"single_color\"], \n",
    "                marker=model_draw_style[model_type]['marker'], \n",
    "                markevery=20, \n",
    "                linestyle=model_draw_style[model_type]['linestyle'])\n",
    "        \n",
    "        # ensemble seed\n",
    "        values = list(models['ensemble'].values())\n",
    "        bin_, cdf_ = ecdf(values)\n",
    "        ax.plot(bin_,\n",
    "                cdf_, \n",
    "                label=f\"{model_draw_style[model_type]['label']} ensemble (n=8)\", \n",
    "                color=model_draw_style[model_type]['ensemble_color'], \n",
    "                linestyle=model_draw_style[model_type]['linestyle'])\n",
    "    elif model_type == \"benchmarks\":\n",
    "        for benchmark_model, benchmark_result in models.items():\n",
    "            if not \"conus\" in benchmark_model:\n",
    "                values = list(benchmark_result.values())\n",
    "                bin_, cdf_ = ecdf(values)\n",
    "                ax.plot(bin_,\n",
    "                        cdf_, \n",
    "                        label=model_draw_style[benchmark_model]['label'], \n",
    "                        color=model_draw_style[benchmark_model]['color'], \n",
    "                        linestyle=model_draw_style[benchmark_model]['linestyle'])\n",
    "    \n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('NSE', fontsize=14)\n",
    "ax.set_ylabel('cumulative density', fontsize=14)\n",
    "ax.set_title(\"Benchmarking against basin-wise calibrated hydrological models\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for model, desc in [(\"ealstm_NSE\", \"EA-LSTM with NSE\"), (\"xgboost_MSE\", \"XGBoost with MSE\")]:\n",
    "    single_model = {'model': desc, 'ensemble': False}\n",
    "    ensemble_mean = {'model': desc, 'ensemble': True}\n",
    "    # get EA-LSTM stats for all metrics\n",
    "    for metric, metric_data in sub_metrics.items():\n",
    "\n",
    "        # average over single models\n",
    "        seeds = [k for k in metric_data[model].keys() if \"seed\" in k]\n",
    "        seed_vals = defaultdict(list)\n",
    "        for seed in seeds:\n",
    "            values = list(metric_data[model][seed].values())\n",
    "            seed_vals[f\"{metric} median\"].append(np.median(values))\n",
    "            if metric == \"NSE\":\n",
    "                seed_vals[f\"{metric} mean\"].append(np.mean(values))\n",
    "                seed_vals[\"failures\"].append(len([v for v in values if v <= 0]))\n",
    "            single_model[f\"{metric} median\"] = np.mean(seed_vals[f\"{metric} median\"])\n",
    "            single_model[f\"{metric} median std\"] = np.std(seed_vals[f\"{metric} median\"])\n",
    "            if metric == \"NSE\":\n",
    "                single_model[f\"{metric} mean\"] = np.mean(seed_vals[f\"{metric} mean\"])\n",
    "                single_model[f\"{metric} mean std\"] = np.std(seed_vals[f\"{metric} mean\"])\n",
    "                single_model[f\"failures\"] = np.mean(seed_vals[\"failures\"])\n",
    "                single_model[f\"failures std\"] = np.std(seed_vals[\"failures\"])\n",
    "\n",
    "        # ensemble mean\n",
    "        values = list(metric_data[model][\"ensemble\"].values())\n",
    "        ensemble_mean[f\"{metric} median\"] = np.median(values)\n",
    "        if metric == \"NSE\":\n",
    "            ensemble_mean[\"NSE mean\"] = np.mean(values)\n",
    "            ensemble_mean[\"failures\"] = len([v for v in values if v <= 0])\n",
    "        \n",
    "    data.append(single_model)\n",
    "    data.append(ensemble_mean)\n",
    "        \n",
    "# benchmark models:\n",
    "for model in model_draw_style.keys():\n",
    "    if \"lstm\" in model or \"xgboost\" in model:\n",
    "        continue\n",
    "    model_data = {\"model\": model_draw_style[model][\"label\"], \"ensemble\": False}\n",
    "    for metric, metric_data in sub_metrics.items():\n",
    "        values = list(metric_data[\"benchmarks\"][model].values())\n",
    "        model_data[f\"{metric} median\"] = np.median(values)\n",
    "        if metric == \"NSE\":\n",
    "            model_data[\"NSE mean\"] = np.mean(values)\n",
    "            model_data[\"failures\"] = len([v for v in values if v <= 0])\n",
    "            \n",
    "    data.append(model_data)\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(keys=[\"model\", \"ensemble\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for metric in sub_metrics.keys():\n",
    "    print(f\"\\n\\n#### {metric}\")\n",
    "    print(f\"Comparison between EA-LSTM (optimized with NSE) and mHm (basin-wise calibrated)\")\n",
    "    ealstm_perf = get_mean_basin_performance(sub_metrics[metric], model=\"ealstm_NSE\")\n",
    "    _, p_val_single = wilcoxon(list(ealstm_perf.values()),\n",
    "                               list(sub_metrics[metric][\"benchmarks\"][\"mHm_basin\"].values()))\n",
    "    _, p_val_ensemble = wilcoxon(list(sub_metrics[metric][\"benchmarks\"][\"mHm_basin\"].values()), \n",
    "                                 list(sub_metrics[metric][\"ealstm_NSE\"][\"ensemble\"].values()))\n",
    "    print(f\"For single models: {p_val_single:.2e}\")\n",
    "    print(f\"For ensemble mean {p_val_ensemble:.2e}\")\n",
    "    d_single = get_cohens_d(list(ealstm_perf.values()),\n",
    "                            list(sub_metrics[metric][\"benchmarks\"][\"mHm_basin\"].values()))\n",
    "    d_ensemble = get_cohens_d(list(sub_metrics[metric][\"benchmarks\"][\"mHm_basin\"].values()), \n",
    "                              list(sub_metrics[metric][\"ealstm_NSE\"][\"ensemble\"].values()))\n",
    "    print(f\"Effect size: Single model d={d_single:.3f}, ensemble mean d={d_ensemble:.3f}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nComparison between EA-LSTM (optimized with NSE) and HBV (upper limit)\")\n",
    "    _, p_val_single = wilcoxon(list(ealstm_perf.values()),\n",
    "                               list(sub_metrics[metric][\"benchmarks\"][\"HBV_ub\"].values()))\n",
    "    _, p_val_ensemble = wilcoxon(list(sub_metrics[metric][\"benchmarks\"][\"HBV_ub\"].values()), \n",
    "                                 list(sub_metrics[metric][\"ealstm_NSE\"][\"ensemble\"].values()))\n",
    "    print(f\"For single models: {p_val_single:.2e}\")\n",
    "    print(f\"For ensemble mean {p_val_ensemble:.2e}\")\n",
    "    d_single = get_cohens_d(list(ealstm_perf.values()),\n",
    "                            list(sub_metrics[metric][\"benchmarks\"][\"HBV_ub\"].values()))\n",
    "    d_ensemble = get_cohens_d(list(sub_metrics[metric][\"benchmarks\"][\"HBV_ub\"].values()), \n",
    "                              list(sub_metrics[metric][\"ealstm_NSE\"][\"ensemble\"].values()))\n",
    "    print(f\"Effect size: Single model d={d_single:.3f}, ensemble mean d={d_ensemble:.3f}\")\n",
    "    \n",
    "    print(f\"\\nComparison between EA-LSTM (optimized with NSE) and XGBoost (optimized with MSE)\")\n",
    "    xgb_perf = get_mean_basin_performance(sub_metrics[metric], model=\"xgboost_MSE\")\n",
    "    _, p_val_single = wilcoxon(list(ealstm_perf.values()),\n",
    "                               list(xgb_perf.values()))\n",
    "    _, p_val_ensemble = wilcoxon(list(sub_metrics[metric][\"xgboost_MSE\"][\"ensemble\"].values()), \n",
    "                                 list(sub_metrics[metric][\"ealstm_NSE\"][\"ensemble\"].values()))\n",
    "    print(f\"For single models: {p_val_single:.2e}\")\n",
    "    print(f\"For ensemble mean {p_val_ensemble:.2e}\")\n",
    "    d_single = get_cohens_d(list(ealstm_perf.values()),\n",
    "                            list(xgb_perf.values()))\n",
    "    d_ensemble = get_cohens_d(list(sub_metrics[metric][\"xgboost_MSE\"][\"ensemble\"].values()), \n",
    "                              list(sub_metrics[metric][\"ealstm_NSE\"][\"ensemble\"].values()))\n",
    "    print(f\"Effect size: Single model d={d_single:.3f}, ensemble mean d={d_ensemble:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
